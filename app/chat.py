"""
Company Q&A Chat Engine.

Optimized with:
- Query caching (LRU, 1hr TTL)
- Streaming responses (SSE)
- TOP_K=3 with 0.4 threshold for better accuracy
- 500 max tokens for concise answers
"""

import hashlib
import json
import logging
import time
from typing import List, Dict, Optional
from collections import OrderedDict

from openai import OpenAI
from pinecone import Pinecone

from app.config import (
    OPENAI_API_KEY,
    EMBEDDING_MODEL,
    CHAT_MODEL,
    PINECONE_API_KEY,
    PINECONE_INDEX_NAME,
    COMPANY_QA_NAMESPACE,
)

logger = logging.getLogger(__name__)

openai_client = OpenAI(api_key=OPENAI_API_KEY)
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(PINECONE_INDEX_NAME)

# â”€â”€ Optimized retrieval config â”€â”€
TOP_K = 3
SIMILARITY_THRESHOLD = 0.4
MAX_TOKENS = 500


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Query Cache (LRU, in-memory)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class QueryCache:
    def __init__(self, max_size=200, ttl_seconds=3600):
        self.max_size = max_size
        self.ttl = ttl_seconds
        self._cache = OrderedDict()

    def _key(self, query):
        return hashlib.md5(query.strip().lower().encode()).hexdigest()

    def get(self, query):
        key = self._key(query)
        if key in self._cache:
            entry = self._cache[key]
            if time.time() - entry["timestamp"] < self.ttl:
                self._cache.move_to_end(key)
                logger.info(f"Cache HIT for query: '{query[:40]}...'")
                return entry["result"]
            else:
                del self._cache[key]
        return None

    def put(self, query, result):
        key = self._key(query)
        if len(self._cache) >= self.max_size:
            self._cache.popitem(last=False)
        self._cache[key] = {"result": result, "timestamp": time.time()}

    def clear(self):
        self._cache.clear()
        logger.info("Query cache cleared")


cache = QueryCache(max_size=200, ttl_seconds=3600)


SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½è¦ªåˆ‡å°ˆæ¥­çš„å®¢æœåŠ©ç†ï¼Œè² è²¬å›ç­”è¨ªå®¢é—œæ–¼å…¬å¸æœå‹™çš„å•é¡Œã€‚

## å›ç­”è¦å‰‡

1. **æ‰“æ‹›å‘¼å’Œå¯’æš„**ï¼šå¦‚æœè¨ªå®¢åªæ˜¯æ‰“æ‹›å‘¼ï¼ˆå¦‚ã€Œä½ å¥½ã€ã€ã€Œå—¨ã€ã€ã€ŒHiã€ï¼‰ï¼Œè«‹è¦ªåˆ‡å›æ‡‰ä¸¦è©¢å•æœ‰ä»€éº¼å¯ä»¥å¹«å¿™çš„ã€‚ä¾‹å¦‚ï¼šã€Œæ‚¨å¥½ï¼å¾ˆé«˜èˆˆç‚ºæ‚¨æœå‹™ ğŸ˜Š è«‹å•æœ‰ä»€éº¼å¯ä»¥å¹«æ‚¨çš„å—ï¼Ÿã€
2. **åƒ…æ ¹æ“šæä¾›çš„åƒè€ƒè³‡æ–™å›ç­”å°ˆæ¥­å•é¡Œ**ã€‚ä¸è¦ç·¨é€ å…¬å¸æ²’æœ‰çš„æœå‹™æˆ–åŠŸèƒ½ã€‚
3. **ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”**ï¼Œèªæ°£è¦ªåˆ‡ã€å°ˆæ¥­ã€‚
4. å¦‚æœåƒè€ƒè³‡æ–™ä¸­æœ‰ç›¸é—œé€£çµï¼Œè‡ªç„¶åœ°åœ¨å›ç­”ä¸­åŒ…å«é€£çµã€‚æ ¼å¼ç¯„ä¾‹ï¼šã€Œæ‚¨å¯ä»¥åœ¨é€™è£¡æŸ¥çœ‹è©³æƒ…ï¼š[é€£çµ]ã€
5. å¦‚æœåƒè€ƒè³‡æ–™ä¸­æ²’æœ‰é€£çµï¼Œå°±æ­£å¸¸å›ç­”æ–‡å­—å…§å®¹å³å¯ï¼Œä¸è¦æåˆ°ã€Œæ²’æœ‰é€£çµã€ã€‚
6. å¦‚æœæœ‰å¤šå€‹ç›¸é—œçµæœï¼Œå°‡å®ƒå€‘æ•´åˆæˆä¸€å€‹å®Œæ•´çš„å›ç­”ï¼Œå¿…è¦æ™‚åˆ—å‡ºé¸é …ã€‚
7. å¦‚æœè¨ªå®¢çš„å•é¡Œå¤ªå»£æ³›ï¼Œå¯ä»¥è©¢å•é€²ä¸€æ­¥çš„ç´°ç¯€ä»¥ç¸®å°ç¯„åœã€‚
8. å¦‚æœæ‰¾ä¸åˆ°ç›¸é—œç­”æ¡ˆï¼Œç¦®è²Œåœ°å›è¦†ï¼šã€Œå¾ˆæŠ±æ­‰ï¼Œæˆ‘ç›®å‰ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€‚è«‹é€é support@example.com æˆ–æ’¥æ‰“ 02-1234-5678 è¯ç¹«æˆ‘å€‘çš„å®¢æœåœ˜éšŠï¼Œæˆ‘å€‘æœƒç›¡å¿«ç‚ºæ‚¨æœå‹™ã€‚ã€
9. ä¿æŒå›ç­”ç°¡æ½”æ˜ç­ï¼Œç”¨2-3å¥è©±å›ç­”ï¼Œé™¤éå•é¡Œéœ€è¦æ›´è©³ç´°çš„è§£é‡‹ã€‚

## é‡è¦
- å°æ‰“æ‹›å‘¼ã€æ„Ÿè¬ã€å†è¦‹ç­‰ç¤¾äº¤æ€§è¨Šæ¯ï¼Œç›´æ¥è¦ªåˆ‡å›æ‡‰å³å¯ï¼Œä¸éœ€è¦åƒè€ƒè³‡æ–™
- çµ•å°ä¸è¦ç·¨é€ ä¸åœ¨åƒè€ƒè³‡æ–™ä¸­çš„è³‡è¨Š
- ä¸è¦å›ç­”èˆ‡å…¬å¸æœå‹™ç„¡é—œçš„å•é¡Œ
- å¦‚æœä¸ç¢ºå®šï¼Œå¯§å¯å¼•å°è¨ªå®¢è¯ç¹«å®¢æœ"""


def embed_query(query):
    response = openai_client.embeddings.create(model=EMBEDDING_MODEL, input=[query])
    return response.data[0].embedding


def retrieve_context(query):
    query_embedding = embed_query(query)
    results = index.query(
        vector=query_embedding, top_k=TOP_K,
        include_metadata=True, namespace=COMPANY_QA_NAMESPACE,
    )
    matches = []
    for match in results.get("matches", []):
        if match["score"] >= SIMILARITY_THRESHOLD:
            matches.append({"score": round(match["score"], 4), "metadata": match["metadata"]})
    logger.info(f"Retrieved {len(matches)} matches for query: '{query[:50]}...'")
    return matches


def build_context_block(matches):
    if not matches:
        return "ï¼ˆæ‰¾ä¸åˆ°ç›¸é—œçš„åƒè€ƒè³‡æ–™ï¼‰"
    blocks = []
    for i, match in enumerate(matches, 1):
        meta = match["metadata"]
        block = f"ã€åƒè€ƒè³‡æ–™ {i}ã€‘ï¼ˆç›¸é—œåº¦ï¼š{match['score']}ï¼‰\n"
        block += f"å•é¡Œï¼š{meta.get('question', '')}\n"
        block += f"ç­”æ¡ˆï¼š{meta.get('answer', '')}"
        if meta.get("link"):
            block += f"\né€£çµï¼š{meta['link']}"
        if meta.get("category"):
            block += f"\nåˆ†é¡ï¼š{meta['category']}"
        blocks.append(block)
    return "\n\n".join(blocks)


def _build_messages(query, context_block, conversation_history=None):
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    if conversation_history:
        messages.extend(conversation_history)
    user_message = f"""è¨ªå®¢å•é¡Œï¼š{query}

ä»¥ä¸‹æ˜¯å¾çŸ¥è­˜åº«ä¸­æª¢ç´¢åˆ°çš„ç›¸é—œåƒè€ƒè³‡æ–™ï¼š

{context_block}

è«‹æ ¹æ“šä»¥ä¸Šåƒè€ƒè³‡æ–™å›ç­”è¨ªå®¢çš„å•é¡Œã€‚"""
    messages.append({"role": "user", "content": user_message})
    return messages


def _calc_confidence(matches):
    if matches:
        avg_score = sum(m["score"] for m in matches) / len(matches)
        return round((avg_score + matches[0]["score"]) / 2, 4)
    return 0.0


def _build_sources(matches):
    sources = []
    for match in matches:
        source = {
            "row_number": match["metadata"].get("row_number"),
            "question": match["metadata"].get("question"),
            "relevance_score": match["score"],
        }
        if match["metadata"].get("category"):
            source["category"] = match["metadata"]["category"]
        if match["metadata"].get("link"):
            source["link"] = match["metadata"]["link"]
        sources.append(source)
    return sources


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Standard (non-streaming) chat
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def chat(query, conversation_history=None):
    start_time = time.time()

    # Check cache
    cached = cache.get(query)
    if cached and not conversation_history:
        cached["latency_seconds"] = round(time.time() - start_time, 3)
        cached["cached"] = True
        return cached

    # Retrieve and generate
    matches = retrieve_context(query)
    context_block = build_context_block(matches)
    messages = _build_messages(query, context_block, conversation_history)

    response = openai_client.chat.completions.create(
        model=CHAT_MODEL, messages=messages,
        temperature=0.3, max_tokens=MAX_TOKENS,
    )

    answer = response.choices[0].message.content
    elapsed = round(time.time() - start_time, 3)

    result = {
        "answer": answer,
        "sources": _build_sources(matches),
        "confidence": _calc_confidence(matches),
        "latency_seconds": elapsed,
        "model": CHAT_MODEL,
        "matches_found": len(matches),
    }

    logger.info(f"Chat response â€” confidence: {result['confidence']}, latency: {elapsed}s")

    if not conversation_history:
        cache.put(query, result)

    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Streaming chat (SSE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def chat_stream(query, conversation_history=None):
    """
    Generator that yields SSE-formatted events:
    - event: metadata (sources, confidence)
    - event: chunk (text pieces as they arrive)
    - event: done (full answer, latency)
    """
    start_time = time.time()

    matches = retrieve_context(query)
    context_block = build_context_block(matches)
    messages = _build_messages(query, context_block, conversation_history)

    confidence = _calc_confidence(matches)
    sources = _build_sources(matches)

    # Yield metadata first so frontend can show sources immediately
    yield f"data: {json.dumps({'type': 'metadata', 'sources': sources, 'confidence': confidence}, ensure_ascii=False)}\n\n"

    # Stream LLM response
    stream = openai_client.chat.completions.create(
        model=CHAT_MODEL, messages=messages,
        temperature=0.3, max_tokens=MAX_TOKENS, stream=True,
    )

    full_answer = ""
    for chunk in stream:
        if chunk.choices[0].delta.content:
            text = chunk.choices[0].delta.content
            full_answer += text
            yield f"data: {json.dumps({'type': 'chunk', 'content': text}, ensure_ascii=False)}\n\n"

    elapsed = round(time.time() - start_time, 3)

    # Cache result
    if not conversation_history:
        result = {
            "answer": full_answer, "sources": sources,
            "confidence": confidence, "latency_seconds": elapsed,
            "model": CHAT_MODEL, "matches_found": len(matches),
        }
        cache.put(query, result)

    # Final event
    yield f"data: {json.dumps({'type': 'done', 'answer': full_answer, 'latency_seconds': elapsed}, ensure_ascii=False)}\n\n"

    logger.info(f"Streamed response â€” confidence: {confidence}, latency: {elapsed}s")


def clear_cache():
    """Clear the query cache. Call after re-indexing."""
    cache.clear()
